{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","root_dir = \"/content/drive/MyDrive/\"\n","#import os\n","#os.chdir(root_dir + 'Project/Final_Dataset/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIGLvpGmNJgP","executionInfo":{"status":"ok","timestamp":1712551347135,"user_tz":-330,"elapsed":32606,"user":{"displayName":"Fathima Sahliya","userId":"00503529692663616407"}},"outputId":"8880e3ec-4030-411e-a499-067e1a59823c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YDhiiz_dJxB4","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1712551549795,"user_tz":-330,"elapsed":8511,"user":{"displayName":"Fathima Sahliya","userId":"00503529692663616407"}},"outputId":"8b0ed4c0-7235-45b9-f930-3ce1a0b11378"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"No file or directory found at /content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-fd3d9e1f1161>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5"]}],"source":["import os\n","import os\n","import re\n","import pandas as pd\n","import pickle\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Concatenate\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import json\n","from tensorflow.keras.models import load_model as ld\n","from tkinter import filedialog\n","import json\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import tokenizer_from_json\n","#from cap_gen import generate_caption\n","\n","\n","\n","def idx_to_word(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate caption for an image\n","def predict_hashtag(model, image, tokenizer, max_length):\n","\t# add start tag for generation process\n","\tin_text = 'startseq'\n","\t# iterate over the max length of sequence\n","\tfor i in range(max_length):\n","\t\t# encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad the sequence\n","\t\tsequence = pad_sequences([sequence], max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([image, sequence], verbose=0)\n","\t\t# get index with high probability\n","\t\tyhat = np.argmax(yhat)\n","\t\t# convert index to word\n","\t\tword = idx_to_word(yhat, tokenizer)\n","\t\t# stop if word not found\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append word as input for generating next word\n","\t\tin_text += \" \" + word\n","\t\t# stop if we reach end tag\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\n","\treturn in_text\n","\n","\n","\n","if __name__ == \"__main__\":\n","\timage_model = InceptionV3(weights='imagenet')\n","\timage_model = Model(inputs=image_model.input, outputs=image_model.layers[-2].output)\n","\n","\twith open('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/tokenizer.json', 'r', encoding='utf-8') as json_file:\n","\t\ttokenizer_json = json_file.read()\n","\n","\ttokenizer = tokenizer_from_json(tokenizer_json)\n","\n","\tmodel = ld('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5')\n","\ttext_model = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\")\n","\n","\ttextt = input(\"Enter some text: \")\n","\n","\ttext_feature = text_model([textt])[0].numpy()\n","\n","\ttext_feature = np.expand_dims(text_feature, axis=0)\n","\n","\timg_path = '/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/artpic.jpg'\n","\t#img_path = '90000.jpg'\n","\timage = load_img(img_path, target_size=(299, 299))\n","\timage = img_to_array(image)\n","\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\timage = preprocess_input(image)\n","\n","\timfeature = image_model.predict(image, verbose=0)\n","\n","\tmerged_feature = np.concatenate([imfeature, text_feature], axis=1)\n","  # Concatenate image features with resized text features\n","\t#merged_feature = np.concatenate([imfeature, resized_text_feature], axis=1)\n","\n","\tpr_h=predict_hashtag(model, merged_feature, tokenizer, 7)\n","\n","\t#print(pr_h)\n","\n","\tpr_h = pr_h.replace(\"startseq\", \"\").replace(\"endseq\", \"\")\n","\n","\twords = pr_h.split()\n","\n","\t# Add '#' to each word\n","\thashtags = ['#' + word.lower() for word in words]\n","\n","\tformatted_output = ' '.join(hashtags)\n","\n","\tprint(formatted_output)\n","\tformatted_output_without_hashtags = formatted_output.replace(\"#\", \"\")\n","\t#print(formatted_output_without_hashtags)\n","\t# predicted_caption = generate_caption(formatted_output_without_hashtags)\n","\t# print(\"Predicted Caption:\", predicted_caption)\n","\n","\t# modified_text = predicted_caption.replace(\"eostok\", \"\")\n","\n","\t# print(modified_text)\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["import os\n","import os\n","import re\n","import pandas as pd\n","import pickle\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Concatenate\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import json\n","from tensorflow.keras.models import load_model as ld\n","from tkinter import filedialog\n","import json\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import tokenizer_from_json\n","#from cap_gen import generate_caption\n","\n","\n","\n","def idx_to_word(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate caption for an image\n","def predict_hashtag(model, image, tokenizer, max_length):\n","\t# add start tag for generation process\n","\tin_text = 'startseq'\n","\t# iterate over the max length of sequence\n","\tfor i in range(max_length):\n","\t\t# encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad the sequence\n","\t\tsequence = pad_sequences([sequence], max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([image, sequence], verbose=0)\n","\t\t# get index with high probability\n","\t\tyhat = np.argmax(yhat)\n","\t\t# convert index to word\n","\t\tword = idx_to_word(yhat, tokenizer)\n","\t\t# stop if word not found\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append word as input for generating next word\n","\t\tin_text += \" \" + word\n","\t\t# stop if we reach end tag\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\n","\treturn in_text\n","\n","\n","\n","if __name__ == \"__main__\":\n","\timage_model = InceptionV3(weights='imagenet')\n","\timage_model = Model(inputs=image_model.input, outputs=image_model.layers[-2].output)\n","\n","\twith open('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/tokenizer.json', 'r', encoding='utf-8') as json_file:\n","\t\ttokenizer_json = json_file.read()\n","\n","\ttokenizer = tokenizer_from_json(tokenizer_json)\n","\n","\tmodel = ld('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5')\n","\ttext_model = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\")\n","\n","\ttextt = input(\"Enter some text: \")\n","\n","\ttext_feature = text_model([textt])[0].numpy()\n","\n","\ttext_feature = np.expand_dims(text_feature, axis=0)\n","\n","\timg_path = '/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/cat.jpg'\n","\t#img_path = '90000.jpg'\n","\timage = load_img(img_path, target_size=(299, 299))\n","\timage = img_to_array(image)\n","\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\timage = preprocess_input(image)\n","\n","\timfeature = image_model.predict(image, verbose=0)\n","\n","\tmerged_feature = np.concatenate([imfeature, text_feature], axis=1)\n","  # Concatenate image features with resized text features\n","\t#merged_feature = np.concatenate([imfeature, resized_text_feature], axis=1)\n","\n","\tpr_h=predict_hashtag(model, merged_feature, tokenizer, 7)\n","\n","\t#print(pr_h)\n","\n","\tpr_h = pr_h.replace(\"startseq\", \"\").replace(\"endseq\", \"\")\n","\n","\twords = pr_h.split()\n","\n","\t# Add '#' to each word\n","\thashtags = ['#' + word.lower() for word in words]\n","\n","\tformatted_output = ' '.join(hashtags)\n","\n","\tprint(formatted_output)\n","\tformatted_output_without_hashtags = formatted_output.replace(\"#\", \"\")\n","\t#print(formatted_output_without_hashtags)\n","\t# predicted_caption = generate_caption(formatted_output_without_hashtags)\n","\t# print(\"Predicted Caption:\", predicted_caption)\n","\n","\t# modified_text = predicted_caption.replace(\"eostok\", \"\")\n","\n","\t# print(modified_text)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOpaxPdm_hQ9","executionInfo":{"status":"ok","timestamp":1712483945814,"user_tz":-330,"elapsed":24165,"user":{"displayName":"AMNN Hashtag Recommendation","userId":"05388680023762936285"}},"outputId":"fd09339a-feb0-465d-93e4-715242409472"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter some text: cutest pet ever\n","#animallover #animal #cat #cute #cutecat\n"]}]},{"cell_type":"code","source":["import os\n","import os\n","import re\n","import pandas as pd\n","import pickle\n","import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Concatenate\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import json\n","from tensorflow.keras.models import load_model as ld\n","from tkinter import filedialog\n","import json\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import tokenizer_from_json\n","#from cap_gen import generate_caption\n","\n","\n","\n","def idx_to_word(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate caption for an image\n","def predict_hashtag(model, image, tokenizer, max_length):\n","\t# add start tag for generation process\n","\tin_text = 'startseq'\n","\t# iterate over the max length of sequence\n","\tfor i in range(max_length):\n","\t\t# encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad the sequence\n","\t\tsequence = pad_sequences([sequence], max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([image, sequence], verbose=0)\n","\t\t# get index with high probability\n","\t\tyhat = np.argmax(yhat)\n","\t\t# convert index to word\n","\t\tword = idx_to_word(yhat, tokenizer)\n","\t\t# stop if word not found\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append word as input for generating next word\n","\t\tin_text += \" \" + word\n","\t\t# stop if we reach end tag\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\n","\treturn in_text\n","\n","\n","\n","if __name__ == \"__main__\":\n","\timage_model = InceptionV3(weights='imagenet')\n","\timage_model = Model(inputs=image_model.input, outputs=image_model.layers[-2].output)\n","\n","\twith open('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/tokenizer.json', 'r', encoding='utf-8') as json_file:\n","\t\ttokenizer_json = json_file.read()\n","\n","\ttokenizer = tokenizer_from_json(tokenizer_json)\n","\n","\tmodel = ld('/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/best_model.h5')\n","\ttext_model = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\")\n","\n","\ttextt = input(\"Enter some text: \")\n","\n","\ttext_feature = text_model([textt])[0].numpy()\n","\n","\ttext_feature = np.expand_dims(text_feature, axis=0)\n","\n","\t# Resize text features to match the expected input shape of the captioning model\n","\ttarget_shape = (1, 2048)  # Assuming the captioning model expects input shape (1, 2048)\n","\t#resized_text_feature = resize_text_features(text_feature, target_shape)\n","\n","\n","\n","\n","\timg_path = '/content/drive/MyDrive/Project/Final_Dataset/notebooks/proposed model/baby.jpg'\n","\t#img_path = '90000.jpg'\n","\timage = load_img(img_path, target_size=(299, 299))\n","\timage = img_to_array(image)\n","\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\timage = preprocess_input(image)\n","\n","\timfeature = image_model.predict(image, verbose=0)\n","\n","\tmerged_feature = np.concatenate([imfeature, text_feature], axis=1)\n","  # Concatenate image features with resized text features\n","\t#merged_feature = np.concatenate([imfeature, resized_text_feature], axis=1)\n","\n","\tpr_h=predict_hashtag(model, merged_feature, tokenizer, 7)\n","\n","\t#print(pr_h)\n","\n","\tpr_h = pr_h.replace(\"startseq\", \"\").replace(\"endseq\", \"\")\n","\n","\twords = pr_h.split()\n","\n","\t# Add '#' to each word\n","\thashtags = ['#' + word.lower() for word in words]\n","\n","\tformatted_output = ' '.join(hashtags)\n","\n","\tprint(formatted_output)\n","\tformatted_output_without_hashtags = formatted_output.replace(\"#\", \"\")\n","\t#print(formatted_output_without_hashtags)\n","\t# predicted_caption = generate_caption(formatted_output_without_hashtags)\n","\t# print(\"Predicted Caption:\", predicted_caption)\n","\n","\t# modified_text = predicted_caption.replace(\"eostok\", \"\")\n","\n","\t# print(modified_text)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxFJyymZ_uyP","executionInfo":{"status":"ok","timestamp":1712490609961,"user_tz":-330,"elapsed":52358,"user":{"displayName":"AMNN Hashtag Recommendation","userId":"05388680023762936285"}},"outputId":"b6381dca-ace7-4140-96d3-d8dd3e4ea9dc"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter some text: babyfever\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x783a6ff03d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["#amazing #holidayseason #pajamaparty #family #yay\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P7JrILzZF_R7"},"execution_count":null,"outputs":[]}]}