{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eaaa84-a7a8-4cd3-bf62-a58f0d687bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\govin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import subprocess\n",
    "from flask_cors import CORS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from werkzeug.serving import run_simple\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Add this line to enable CORS for all routes\n",
    "\n",
    "\n",
    "@app.route('/predictCO', methods=['POST'])\n",
    "\n",
    "def predictCO():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelco.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted CO values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35605fd-fe8b-4042-9b8f-bddca901e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.route('/predictNO2', methods=['POST'])\n",
    "\n",
    "def predictNO2():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelno2.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted NO2 values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e1ddb3-0f67-41ba-a0a2-36460d25359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.route('/predictO3', methods=['POST'])\n",
    "\n",
    "def predictO3():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelo3.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted O3 values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6a4252-6e1d-4441-9bf1-c77679ad9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predictSO2', methods=['POST'])\n",
    "\n",
    "def predictSO2():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelso2.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted SO2 values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e189fd80-993c-41c4-9481-a1e7724a3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predictPM10', methods=['POST'])\n",
    "\n",
    "def predictPM10():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelpm10.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted PM10 values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191ddf75-4f2d-413e-b774-ffc74da88499",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predictPM25', methods=['POST'])\n",
    "\n",
    "def predictPM25():\n",
    "    print(\"API server to run the machine learning model has been started\")\n",
    "    # Receive input data\n",
    "    data = request.get_json()\n",
    "    data=json.loads(data['data'])\n",
    "    print(data)\n",
    "\n",
    "\n",
    "    # # Specify the file path\n",
    "    # file_path = 'C:\\\\Users\\\\govin\\\\Downloads\\\\model1.h5'\n",
    "\n",
    "    # # Check file access permissions\n",
    "    # if os.access(file_path, os.R_OK):\n",
    "    #     print( \"File is readable\")\n",
    "    # else:\n",
    "    #     print( \"File is not readable\")\n",
    "\n",
    "\n",
    "\n",
    "    # Load the saved model\n",
    "    with open('modelpm25.pkl', 'rb') as f:\n",
    "\n",
    "        model = pickle.load(f)\n",
    "    print(\"model has been loaded\")\n",
    "\n",
    "    # Define the scaler with the same feature range used during training\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # User input for three values\n",
    "\n",
    "    # { \"one\" : 12 , \"two\" : 12, \"three\" : 12}\n",
    "    user_input_values = [data['one'],data['two'],data['three']]  # Example user input\n",
    "    print(user_input_values)\n",
    "\n",
    "    # Normalize the user input using the scaler\n",
    "    user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "    # Prepare the user input data for prediction\n",
    "    n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "    n_features = 1\n",
    "    X_user_input = []\n",
    "    X_user_input.append(user_input_norm.flatten())\n",
    "    X_user_input = np.array(X_user_input)\n",
    "    X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(X_user_input)\n",
    "\n",
    "    # Reverse the scaling on the predictions to get the actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    output = predictions[0][0]\n",
    "    output=float(output)\n",
    "    # Print the predictions\n",
    "    print(\"Predicted PM2.5 values based on user input:\")\n",
    "    print(output)\n",
    "    # Save input data to a file (if needed)\n",
    "    #with open('input_data.json', 'w') as f:\n",
    "    #    json.dump(data, f)\n",
    "\n",
    "    # Run Python script\n",
    "    #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "    # Extract output\n",
    "    #output = result.stdout.decode()\n",
    "\n",
    "    #output = \"This is output from local python server for running machine learning model\";\n",
    "\n",
    "    #print(jsonify({'output': output}))\n",
    "    \n",
    "    return jsonify({'output': output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014da9d0-74ee-4821-8250-2760b6217ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/Apr/2024 20:24:43] \"OPTIONS /predictCO HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server to run the machine learning model has been started\n",
      "{'one': 29, 'two': 29, 'three': 27}\n",
      "WARNING:tensorflow:From C:\\Users\\govin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\govin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "model has been loaded\n",
      "[29, 29, 27]\n",
      "1/1 [==============================] - 10s 10s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:24:54] \"POST /predictCO HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted CO values based on user input:\n",
      "29.04779815673828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:24:58] \"OPTIONS /predictNO2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server to run the machine learning model has been started\n",
      "{'one': 778.64, 'two': 751.76, 'three': 636.9}\n",
      "model has been loaded\n",
      "[778.64, 751.76, 636.9]\n",
      "1/1 [==============================] - 1s 561ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:24:58] \"POST /predictNO2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted NO2 values based on user input:\n",
      "649.008056640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:25:00] \"OPTIONS /predictO3 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server to run the machine learning model has been started\n",
      "{'one': 29.8, 'two': 29.9, 'three': 29.9}\n",
      "model has been loaded\n",
      "[29.8, 29.9, 29.9]\n",
      "1/1 [==============================] - 1s 898ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:25:01] \"POST /predictO3 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted O3 values based on user input:\n",
      "29.85735511779785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:25:03] \"OPTIONS /predictSO2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server to run the machine learning model has been started\n",
      "{'one': 29, 'two': 28, 'three': 27}\n",
      "model has been loaded\n",
      "[29, 28, 27]\n",
      "1/1 [==============================] - 1s 549ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Apr/2024 20:25:04] \"POST /predictSO2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SO2 values based on user input:\n",
      "27.327320098876953\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 5000, app)\n",
    "    #app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba8d0a1-57bd-4fde-aa01-6210026e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tensorflow.keras import initializers  # Import the initializers module\n",
    "# from tensorflow.keras.initializers import Initializer  # Import Initializer\n",
    "# from keras.models import load_model\n",
    "# import pickle\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Specify the file path\n",
    "# file_path = 'C:/Users/govin/Downloads'\n",
    "\n",
    "\n",
    "# # # Define custom objects\n",
    "# # custom_objects = {'Orthogonal': keras.initializers.Orthogonal}\n",
    "\n",
    "# # # Load the saved model with custom objects\n",
    "# # model = load_model(file_path, custom_objects=custom_objects)\n",
    "# # print(\"Model has been loaded\")\n",
    "\n",
    "\n",
    "\n",
    "# # Load the saved model\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "\n",
    "#     model = pickle.load(f)\n",
    "# print(\"model has been loaded\")\n",
    "\n",
    "\n",
    "# # Define the scaler with the same feature range used during training\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# # User input for three values\n",
    "# user_input_values = [data.one,data.two,data.three]  # Example user input\n",
    "\n",
    "# # Normalize the user input using the scaler\n",
    "# user_input_norm = scaler.fit_transform(np.array(user_input_values).reshape(-1, 1))\n",
    "\n",
    "# # Prepare the user input data for prediction\n",
    "# n_steps = 3  # Assuming the model was trained with 3 steps\n",
    "# n_features = 1\n",
    "# X_user_input = []\n",
    "# X_user_input.append(user_input_norm.flatten())\n",
    "# X_user_input = np.array(X_user_input)\n",
    "# X_user_input = np.reshape(X_user_input, (X_user_input.shape[0], X_user_input.shape[1], n_features))\n",
    "\n",
    "# # Make predictions using the loaded model\n",
    "# predictions = model.predict(X_user_input)\n",
    "\n",
    "# # Reverse the scaling on the predictions to get the actual values\n",
    "# predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# # Print the predictions\n",
    "# print(\"Predicted CO values based on user input:\")\n",
    "# print(predictions)\n",
    "# # Save input data to a file (if needed)\n",
    "# #with open('input_data.json', 'w') as f:\n",
    "# #    json.dump(data, f)\n",
    "\n",
    "# # Run Python script\n",
    "# #result = subprocess.run(['python', 'your_script.py'], capture_output=True)\n",
    "\n",
    "# # Extract output\n",
    "# #output = result.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d346416-3180-486c-a083-9945041c4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.models import load_model\n",
    "\n",
    "# # Load the saved model\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# # Load your new data for prediction into a DataFrame\n",
    "# # For example:\n",
    "# new_data = pd.read_csv('data2.csv')\n",
    "\n",
    "# # Preprocess the new data (similar to how you preprocessed your training data)\n",
    "# # Assuming 'PM2.5' is the target variable\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# new_data_norm = scaler.fit_transform(new_data[['CO']])  # Assuming 'PM2.5' is the target variable\n",
    "\n",
    "# # Prepare the new data for prediction (similar to how you prepared your training data)\n",
    "# n_steps = 3  # Assuming you used 3 steps in your training data\n",
    "# n_features = 1\n",
    "# X_new = []\n",
    "# for i in range(len(new_data_norm) - n_steps + 1):\n",
    "#     X_new.append(new_data_norm[i:i+n_steps, 0])\n",
    "# X_new = np.array(X_new)\n",
    "# X_new = np.reshape(X_new, (X_new.shape[0], X_new.shape[1], n_features))\n",
    "\n",
    "# # Make predictions using the loaded model\n",
    "# predictions = model.predict(X_new)\n",
    "\n",
    "# # Reverse the scaling on the predictions to get the actual values\n",
    "# predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# # Assuming 'PM2.5' is the target variable, you can now use 'predictions' for your analysis\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf343e4b-4d06-463d-a161-608309ef876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         function predictCO() {\n",
    "#             // Get input data\n",
    "#             var inputData = document.getElementById('inputData').value;\n",
    "#         console.log(inputData);\n",
    "#             // Make a POST request to the API\n",
    "#     fetch('http://127.0.0.1:5000/predictCO', {\n",
    "#         method: 'POST',\n",
    "#         headers: {\n",
    "#             'Content-Type': 'application/json'\n",
    "#         },\n",
    "#         body: JSON.stringify({data: inputData})\n",
    "#     })\n",
    "#     .then(response => response.json())\n",
    "#     .then(data => {\n",
    "#         // Display output\n",
    "#         console.log(data)\n",
    "#         document.getElementById('output').innerText = data.output;\n",
    "#     })\n",
    "#     .catch(error => {\n",
    "#         console.error('Error:', error);\n",
    "#     });\n",
    "# }\n",
    "# function predictNO2() {\n",
    "#             // Get input data\n",
    "#             var inputData = document.getElementById('inputData').value;\n",
    "#         console.log(inputData);\n",
    "#             // Make a POST request to the API\n",
    "#     fetch('http://127.0.0.1:5000/predictNO2', {\n",
    "#         method: 'POST',\n",
    "#         headers: {\n",
    "#             'Content-Type': 'application/json'\n",
    "#         },\n",
    "#         body: JSON.stringify({data: inputData})\n",
    "#     })\n",
    "#     .then(response => response.json())\n",
    "#     .then(data => {\n",
    "#         // Display output\n",
    "#         console.log(data)\n",
    "#         document.getElementById('output').innerText = data.output;\n",
    "#     })\n",
    "#     .catch(error => {\n",
    "#         console.error('Error:', error);\n",
    "#     });\n",
    "# }\n",
    "# function predictO3() {\n",
    "#             // Get input data\n",
    "#             var inputData = document.getElementById('inputData').value;\n",
    "#         console.log(inputData);\n",
    "#             // Make a POST request to the API\n",
    "#     fetch('http://127.0.0.1:5000/predictO3', {\n",
    "#         method: 'POST',\n",
    "#         headers: {\n",
    "#             'Content-Type': 'application/json'\n",
    "#         },\n",
    "#         body: JSON.stringify({data: inputData})\n",
    "#     })\n",
    "#     .then(response => response.json())\n",
    "#     .then(data => {\n",
    "#         // Display output\n",
    "#         console.log(data)\n",
    "#         document.getElementById('output').innerText = data.output;\n",
    "#     })\n",
    "#     .catch(error => {\n",
    "#         console.error('Error:', error);\n",
    "#     });\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
